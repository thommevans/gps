import sys, os, pdb, time
import gp_routines, spgp_routines
import numpy as np

PERTURB = 1e-4

class gp():
    """
    Class for full and sparse GP objects.
    """

    def __init__( self, which_type='full' ):
        """
        Initialises an instance of the gp class.
        """
        self.xtrain = None
        self.dtrain = None
        self.etrain = None
        if which_type=='full':
            self.spgp = False
        elif which_type=='sparse':
            self.spgp = True
        self.xinduc = None
        self.mfunc = None
        self.mpars = None
        self.cfunc = None
        self.cpars = None


    def random_draw( self, xmesh=None, emesh=None, conditioned=True, perturb=PERTURB, ndraws=5, \
                     plot_draws=True, mesh_dim=0, lw=3 ):
        """
        SUMMARY

        Draws one or more random realisations from the gp and (optionally) plots them,
        along with the mean function (black dashed line) and 1- and 2-sigma uncertainty
        regions (shaded grey regions).

        CALLING
    
        draws = gp_obj.random_draw( xmesh=None, emesh=None, conditioned=True, perturb=PERTURB, \
                                    ndraws=5, plot_draws=True, mesh_dim=0, lw=3 )

        INPUTS
    
        'xmesh' [KxD array] - input locations for the random draw points; if set to
            None (default), a fine grid spanning the xtrain range will be used.
        'emesh' [float] -  white noise value for the random draw points; if set to
            None (default) or zero, then this will be set to the value of the perturb
            variable for numerical stability.
        'conditioned' [bool] - if set to True (default), the GP will be trained on the 
            training data stored in the object; otherwise, it will be drawn from the
            unconditioned prior.
        'perturb' [float] - small perturbation to be added to the covariance diagonal for
            numerical stability if the white noise errors are set to None/zero.
        'ndraws' [integer] - the number of random draws to be made.
        'plot_draws' [bool] - if set to True, the random draws will be plotted.
        'mesh_dim' [integer] - for cases where D>1 (i.e. multidimensional input), a single
            input dimension must be specified for the mesh to span; the other input 
            variables will be held fixed to the corresponding median values in the training
            data set.
        'lw' [integer] - thickness of plot lines.
      
        OUTPUT
    
        'draws' [list] - a list containing the separate random draws from the GP, each of
             which is a numpy array.

        """

        if self.spgp==False:
            draws = gp_routines.random_draw( self, xmesh=xmesh, emesh=emesh, conditioned=conditioned, \
                                             perturb=perturb, ndraws=ndraws, plot_draws=plot_draws, \
                                             mesh_dim=mesh_dim, lw=lw )
        else:
            draws = spgp_routines.random_draw( self, xmesh=xmesh, emesh=emesh, conditioned=conditioned, \
                                             perturb=perturb, ndraws=ndraws, plot_draws=plot_draws, \
                                             mesh_dim=mesh_dim, lw=lw )
        return draws


    def logp( self, resids=None, sigw=None, Kn=None, knn=None, Km=None, Kmn=None, perturb=PERTURB ):
        """
        SUMMARY

        Evaluates the log likelihood of residuals that are assumed to be generated by a
        gp with a specified covariance. The mean and covariance are passed directly into
        the function as inputs, to allow flexibility in how they are actually computed.
        This can be useful when repeated evaluations of logp are required (eg. likelihood
        maximisation or MCMC), as it may be possible to optimise how these precomputations
        are done outside the function.

        CALLING

        loglikelihood = logp( resids, Kn, sigw, perturb=PERTURB )

        INPUTS

        'resids' [Nx1 array] - residuals between the training data and the gp mean function.
        'Kn' [NxN array] - the covariance matrix between the training inputs.
        'sigw' [float] - white noise value to be incorporated into the covariance diagonal;
              if set to None or zero, it will be set to the value of the perturb variable
              for numerical stability.
        'perturb' [float] - small perturbation to be added to the covariance diagonal for
              numerical stability if the white noise errors are set to None/zero.

        OUTPUT

        'loglikelihood' [float] - the gp log likelihood.
        """

        if self.spgp==False:
            # Full GP:
            loglikelihood = gp_routines.logp( resids, Kn, sigw, perturb=perturb )
        else:
            # Sparse GP:
            loglikelihood = spgp_routines.logp( resids, Km, Kmn, knn, sigw, perturb=perturb )

        return loglikelihood


    def logp_builtin( self, perturb=PERTURB ):
        """
        Uses the contents of the gp object to calculate its log likelihood. The
        logp() routine is actually used to perform the calculation. The difference
        between logp_builtin() and logp() is that the latter requires variables
        such as the covariance matrix to be pre-computed and passed explicitly in
        as arguments - this can be convenient when the covariance matrix is fixed
        such as for type-II maximum likelihood, so that unnecessary recomputing of
        the covariance matrix at each logp evaluation is avoided. But if re-building
        the covariance matrix at each logp evaluation is unavoidable or simply not
        a significant time drag, then logp_builtin() is the simplest way of calculating
        the GP log likelihood, as it requires no external arguments:
            >> logp = gp.logp_builtin()
        """

        if self.spgp==False:
            # Full GP:
            loglikelihood = gp_routines.logp_builtin( self, perturb=perturb )
        else:
            # Sparse GP:
            loglikelihood = spgp_routines.logp_builtin( self, perturb=perturb )
            
        return loglikelihood


    def prep_fixedcov( self, perturb=PERTURB ):
        """
        Prepares a dictionary containing variables that remain unchanged in calculating
        the log likelihood when the covariance parameters are fixed. The usage of this
        routine is along the lines of:
          >> resids = data - model
          >> kwpars = gp.prep_fixedcov()
          >> logp = gp.logp_fixedcov( resids=resids, kwpars=kwpars )
        """
        
        if self.spgp==False:
            # Full GP:
            kwpars = gp_routines.prep_fixedcov( self, perturb=perturb )
        else:
            # Sparse GP:
            kwpars = spgp_routines.prep_fixedcov( self, perturb=perturb )

        return kwpars
        

    def logp_fixedcov( self, resids=None, kwpars=None ):
        """
        Calculates the log likehood using a specific dictionary of arguments that
        are generated using the prep_fixedcov() routine. This routine is used to
        avoid re-calculating the components of the log likelihood that remain
        unchanged if the covariance parameters are fixed, which can potentially
        save time for things like type-II maximum likelihood. The usage of this
        routine is along the lines of:
          >> resids = data - model
          >> kwpars = gp.prep_fixedcov()
          >> logp = gp.logp_fixedcov( resids=resids, kwpars=kwpars )
        """

        if self.spgp==False:
            # Full GP:
            loglikelihood = gp_routines.logp_fixedcov( resids=resids, kwpars=kwpars )
        else:
            # Sparse GP:
            loglikelihood = spgp_routines.logp_fixedcov( resids=resids, kwpars=kwpars )

        return loglikelihood
    

    def meancov( self, xnew=None, enew=None, conditioned=True, add_whitenoise=True, perturb=PERTURB ):
        """
        SUMMARY

        Returns the mean and full covariance of a gp at the locations of xnew, with
        random errors enew. If conditioned==True, the gp will be conditioned on the
        training data stored in the gp_obj. If etrain==None or etrain==0 (stored within
        gp_obj), a perturbation term of magnitude perturb will be added to the diagonal
        entries of the training covariance matrix before it is inverted for numerical
        stability.

        CALLING

        mu, cov = meancov( gp_obj, xnew=None, enew=None, conditioned=True, perturb=PERTURB )

        INPUTS

        'gp_obj' [gp class object], containing:
                  'mfunc', 'cfunc' [functions] - mean and covariance functions.
                  'mpars', 'cpars' [dictionaries] - mean and covariance function parameters. 
                  'xtrain' [NxD array] - training data input locations.
                  'dtrain' [Nx1 array] - training data values.
                  'etrain' [float] - white noise value for the training data points.
        'xnew' [PxD array] - input locations for the mean and covariance to be evaluated at;
              if set to None (default), the values for xtrain will be used.
        'enew' [float] - white noise value to be incorporated into the covariance diagonal;
              if set to None (default) or zero, it will be set to the value of the perturb
              variable for numerical stability.
        'conditioned' [bool] - if set to True (default), the gp will be trained on the
              training data stored in the object.
        'perturb' [float] - small perturbation to be added to the covariance diagonal for
              numerical stability if the white noise errors are set to None/zero.

        OUTPUT

        'mu' [Px1 array] - gp mean function values.
        'cov' [PxP array] - gp covariance values.
        """
        
        if self.spgp==False:
            mu, cov = gp_routines.meancov( self, xnew=xnew, enew=enew, conditioned=conditioned, perturb=perturb )
        else:
            mu, cov = spgp_routines.meancov( self, xnew=xnew, enew=enew, conditioned=conditioned, perturb=perturb )
            
        return mu, cov


    def predictive( self, xnew=None, enew=None, conditioned=True, perturb=PERTURB ):
        """
        SUMMARY

        Returns the predictive mean and standard deviation of a gp.  If conditioned==True,
        the gp will be conditioned on the training data stored in the gp_obj. If
        etrain==None or etrain==0 (stored within gp_obj), a perturbation term of magnitude
        perturb will be added to the diagonal entries of the training covariance matrix
        before it is inverted for numerical stability. This routine is very similar to
        meancov, except that it only calculates the diagonal entries of the conditioned
        gp's covariance matrix to save time.

        CALLING

        mu, sig = gp_obj.predictive( xnew=None, enew=None, conditioned=True, perturb=PERTURB )

        INPUTS

        'gp_obj' [gp class object], containing:
                  'mfunc', 'cfunc' [functions] - mean and covariance functions.
                  'mpars', 'cpars' [dictionaries] - mean and covariance function parameters. 
                  'xtrain' [NxD array] - training data input locations.
                  'dtrain' [Nx1 array] - training data values.
                  'etrain' [float] - white noise value for the training data points.
        'xnew' [PxD array] - input locations for the mean and covariance to be evaluated at;
              if set to None (default), the values for xtrain will be used.
        'enew' [float] - white noise value to be incorporated into the covariance diagonal;
              if set to None (default) or zero, it will be set to the value of the perturb
              variable for numerical stability.
        'conditioned' [bool] - if set to True (default), the gp will be trained on the
              training data stored in the object.
        'perturb' [float] - small perturbation to be added to the covariance diagonal for
              numerical stability if the white noise errors are set to None/zero.

        OUTPUT

        'mu' [Px1 array] - gp mean function values.
        'sig' [Px1 array] - 1-sigma marginalised uncertainties, i.e. the square roots of
              the entries along the diagonal of the full covariance matrix.
        """

        if self.spgp==False:
            mu, sig = gp_routines.predictive( self, xnew=xnew, enew=enew, \
                                              conditioned=conditioned, perturb=perturb )
        else:
            mu, sig = spgp_routines.predictive( self, xnew=xnew, enew=enew, \
                                                conditioned=conditioned, perturb=perturb )

        return mu, sig
